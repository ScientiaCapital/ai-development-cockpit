# ============================================================================
# AI DEVELOPMENT COCKPIT - UNIFIED CONFIGURATION
# Integrates: AI Cost Optimizer + LLM Platform + Dual-Domain System
# ============================================================================

# ----------------------------------------------------------------------------
# CORE LLM PROVIDERS (Cost Optimization Tiers)
# ----------------------------------------------------------------------------

# Tier 1: Free/Ultra-Low-Cost (Simple Queries - 70% of traffic)
GOOGLE_API_KEY="your_google_api_key_here"                 # Required: Gemini Flash (free tier)
                                                           # Get key: https://makersuite.google.com/app/apikey

# Tier 2: Mid-Cost (Complex Queries - 25% of traffic)
ANTHROPIC_API_KEY="your_anthropic_api_key_here"           # Required: Claude Haiku/Sonnet
                                                           # Format: sk-ant-api03-...
OPENROUTER_API_KEY="YOUR_OPENROUTER_KEY_HERE"             # Required: Multi-model fallback (40+ models)
                                                           # Get key: https://openrouter.ai/keys

# Tier 3: Premium (Specialized Chinese LLMs - 5% of traffic)
RUNPOD_API_KEY="your_runpod_api_key_here"                 # Required: Qwen, DeepSeek, ChatGLM
                                                           # Get key: https://runpod.io/console/user/settings
HUGGINGFACE_API_KEY="your_huggingface_api_key_here"       # Required: Model discovery
                                                           # Get key: https://huggingface.co/settings/tokens

# Experimental/Optional
CEREBRAS_API_KEY="your_cerebras_api_key_here"             # Optional: Ultra-fast inference (experimental)
                                                           # Get key: https://cloud.cerebras.ai/

# ----------------------------------------------------------------------------
# ADDITIONAL LLM PROVIDERS (Optional - for research/fallback)
# ----------------------------------------------------------------------------
PERPLEXITY_API_KEY="your_perplexity_api_key_here"         # Optional: Research mode with web search
                                                           # Format: pplx-...
OPENAI_API_KEY="your_openai_api_key_here"                 # Optional: GPT-4o, GPT-4o-mini
                                                           # Format: sk-proj-...
MISTRAL_API_KEY="your_mistral_key_here"                   # Optional: Mistral models
GROQ_API_KEY="YOUR_GROQ_KEY_HERE"                         # Optional: Groq ultra-fast inference
XAI_API_KEY="YOUR_XAI_KEY_HERE"                           # Optional: xAI Grok models
OLLAMA_API_KEY="your_ollama_api_key_here"                 # Optional: Local Ollama authentication
AZURE_OPENAI_API_KEY="your_azure_key_here"                # Optional: Azure OpenAI (requires endpoint)

# ----------------------------------------------------------------------------
# AUTHENTICATION & DATABASE
# ----------------------------------------------------------------------------
NEXT_PUBLIC_SUPABASE_URL="your_supabase_project_url_here" # Required: Supabase project URL
                                                           # Format: https://xxxxx.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY="your_supabase_anon_key_here" # Required: Supabase anonymous key
SUPABASE_SERVICE_ROLE_KEY="your_service_role_key_here"    # Required: Admin operations & migrations

# ----------------------------------------------------------------------------
# COST OPTIMIZER CONFIGURATION (ai-cost-optimizer microservice integration)
# ----------------------------------------------------------------------------
# The ai-cost-optimizer is a SEPARATE microservice deployed independently
# GitHub: https://github.com/ScientiaCapital/ai-cost-optimizer
# This service routes AI requests to the most cost-effective provider
# providing 90% cost savings (Gemini free tier → Claude Haiku → Premium)
# ----------------------------------------------------------------------------

# AI Cost Optimizer Service (FastAPI on port 8000)
# Used by /api/chat endpoint for conversational AI routing
# This is the CHAT-specific endpoint for real-time user conversations
COST_OPTIMIZER_URL="http://localhost:8000"                    # Required: Chat API uses this
                                                           # Production: Your deployed FastAPI URL
                                                           # Development: http://localhost:8000 (local FastAPI service)

# Legacy Cost Optimizer API (if using separate deployment)
# Used by AgentOrchestrator for build-time AI routing (agent coordination)
# This is the BUILD-specific endpoint for agent orchestration
COST_OPTIMIZER_API_URL="https://your-cost-optimizer-vercel-url.vercel.app"  # Required: Agent system uses this
                                                           # Production: Your Vercel deployment URL
                                                           # Development: http://localhost:3002 (if running locally)

COST_OPTIMIZER_API_KEY="your_cost_optimizer_api_key_here" # Required: API key for cost optimizer service
                                                           # Generate from ai-cost-optimizer dashboard

# ----------------------------------------------------------------------------
# RUNPOD CONFIGURATION (Serverless Chinese LLMs)
# ----------------------------------------------------------------------------
RUNPOD_API_ENDPOINT="https://api.runpod.io/v2"           # RunPod API base URL
RUNPOD_WORKSPACE_ID="your_workspace_id_here"             # RunPod workspace identifier
NEXT_PUBLIC_RUNPOD_API_KEY="your_runpod_api_key_here"   # Client-side RunPod key (public)

# RunPod vLLM Configuration
RUNPOD_VLLM_ENDPOINT="your_vllm_endpoint_here"          # vLLM serverless endpoint ID
RUNPOD_DEFAULT_GPU="NVIDIA_A100"                         # GPU type for deployments
RUNPOD_MAX_WORKERS="3"                                   # Max concurrent workers

# ----------------------------------------------------------------------------
# AGENT INFRASTRUCTURE (LangGraph + E2B Sandbox)
# ----------------------------------------------------------------------------
# LangSmith - Agent observability and tracing
LANGCHAIN_API_KEY="lsv2_pt_your_langsmith_key_here"      # Required: LangSmith API key
                                                          # Get key: https://smith.langchain.com/settings
LANGCHAIN_TRACING_V2="true"                              # Enable LangSmith tracing
LANGCHAIN_PROJECT="coperniq-agents"                      # Project name in LangSmith

# E2B Sandbox - Secure code execution for agents
E2B_API_KEY="e2b_your_e2b_key_here"                      # Required: E2B sandbox API key
                                                          # Get key: https://e2b.dev/dashboard
                                                          # Docs: https://e2b.dev/docs

# ----------------------------------------------------------------------------
# MONITORING & ANALYTICS
# ----------------------------------------------------------------------------
PROMETHEUS_PORT="9090"                                    # Prometheus metrics port
GRAFANA_PORT="3000"                                       # Grafana dashboard port
LOG_LEVEL="INFO"                                          # Logging level: DEBUG | INFO | WARN | ERROR
ENABLE_COST_ALERTS="true"                                 # Enable cost threshold alerts (email/webhook)
COST_ALERT_WEBHOOK="your_webhook_url_here"               # Optional: Webhook for cost alerts

# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318"      # OpenTelemetry collector endpoint
OTEL_SERVICE_NAME="ai-development-cockpit"               # Service name for traces

# ----------------------------------------------------------------------------
# ORGANIZATION CONFIGURATION (Dual-Domain Support)
# ----------------------------------------------------------------------------
SWAGGYSTACKS_ORG_ID="swaggystacks"                       # SwaggyStacks organization (developer theme)
SCIENTIA_ORG_ID="scientia-capital"                       # Scientia Capital organization (enterprise theme)

# Organization-specific cost budgets
SWAGGYSTACKS_DAILY_BUDGET="2.00"                         # SwaggyStacks daily budget
SCIENTIA_DAILY_BUDGET="10.00"                            # Scientia Capital daily budget (higher tier)

# ----------------------------------------------------------------------------
# REDIS CACHE CONFIGURATION (Optional but recommended)
# ----------------------------------------------------------------------------
REDIS_URL="redis://localhost:6379"                       # Redis connection URL
REDIS_PASSWORD="your_redis_password_here"                # Redis password (if required)
REDIS_CACHE_TTL="3600"                                   # Cache TTL in seconds (1 hour)

# ----------------------------------------------------------------------------
# OPTIONAL: DEVELOPMENT & INTEGRATION TOOLS
# ----------------------------------------------------------------------------
GITHUB_API_KEY="your_github_api_key_here"                # Optional: GitHub integration & deployments
                                                           # Format: ghp_... or github_pat_...

# Next.js Configuration
NEXT_PUBLIC_APP_URL="http://localhost:3001"              # Public app URL
NODE_ENV="development"                                    # Environment: development | production | test

# Feature Flags
ENABLE_PWA="true"                                         # Enable Progressive Web App features
ENABLE_WEBSOCKETS="true"                                  # Enable real-time updates via WebSockets
ENABLE_CHINESE_LLMS="true"                                # Enable Chinese LLM support (Qwen, DeepSeek, etc.)
ENABLE_MARKETPLACE="true"                                 # Enable model marketplace

# ============================================================================
# NOTES:
# ============================================================================
# 1. Architecture Overview:
#    - This is an AI DEVELOPMENT COCKPIT - orchestrates AI agents to build software
#    - Integrates with ai-cost-optimizer microservice for 90% cost savings
#    - LangGraph-based multi-agent system with human-in-the-loop approval gates
#    - Transforms noobs into software engineering managers
#
# 2. Cost Optimization Strategy (via ai-cost-optimizer microservice):
#    - 70% of queries route to Gemini Flash (FREE via ai-cost-optimizer)
#    - 25% route to Claude Haiku (~$0.13/day)
#    - 5% route to Premium models when needed
#    - Expected monthly cost: $4-5 (vs $45-50 without optimization)
#
# 3. Required API Keys (Minimum to start):
#    - COST_OPTIMIZER_API_URL + API_KEY (ai-cost-optimizer service)
#    - NEXT_PUBLIC_SUPABASE_URL + ANON_KEY (authentication & database)
#
# 4. ai-cost-optimizer Setup:
#    - Deploy ai-cost-optimizer separately to Vercel
#    - GitHub: https://github.com/ScientiaCapital/ai-cost-optimizer
#    - Configure API keys in that service (GOOGLE_API_KEY, ANTHROPIC_API_KEY, etc.)
#    - This cockpit only needs the cost optimizer's URL and API key
#
# 5. Optional but Recommended:
#    - RUNPOD_API_KEY (for Chinese LLM support)
#    - REDIS_URL (for caching and performance)
#
# 6. Security Best Practices:
#    - Never commit this file with real keys
#    - Use .env.local for actual credentials (gitignored)
#    - Rotate keys regularly
#    - The ai-cost-optimizer handles LLM API keys - this service only talks to optimizer
#
# ============================================================================

# ----------------------------------------------------------------------------
# GITHUB OAUTH (for repository integration)
# ----------------------------------------------------------------------------
# Create GitHub OAuth App at: https://github.com/settings/developers
# Callback URL: https://xucngysrzjtwqzgcutqf.supabase.co/auth/v1/callback
GITHUB_CLIENT_ID="your_github_oauth_app_client_id"
GITHUB_CLIENT_SECRET="your_github_oauth_app_client_secret"

# Public Site URL (for OAuth redirects)
NEXT_PUBLIC_SITE_URL="http://localhost:3001"  # Change to production URL when deploying